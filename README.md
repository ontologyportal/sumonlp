# Install pre-requisites


Follow install instructions at https://docs.anaconda.com/miniconda/install/ . Linux instructions reproduced here for convenience:
```
mkdir -p ~/miniconda3
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh
bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3
rm ~/miniconda3/miniconda.sh
source ~/miniconda3/bin/activate
conda init --all
```

Create a conda environment. This allows you to create, export, list, remove, and update environments that have different versions of Python and/or packages installed in them.

As an example: If your version of Python were 3.9, then you would run:
```
conda create -n name_you_choose_for_environment python=3.9
```

Add the name of your environment ("name_you_choose_for_environment") to src/config_paths.sh

## Configuring the prover

The language to logic translator produces the file SUMO_NLP.kif, with logic statements generated from natural language. This file is automatically copied to the .sigmakee/KBs directory. To be able to translate the generated SUO-KIF logic to the TPTP input required by the vampire prover, update 

```
$HOME/.sigmakee/KBs/config.xml
```

and add the following line is in the <kb name="SUMO" > section:

```
...
  <kb name="SUMO" >
    ...
    <constituent filename="/home/THE_USER/.sigmakee/KBs/SUMO_NLP.kif" />
    ...
  </kb>
...

```

The files within this tag are all combined and translated into the SUMO.fof file which is then fed as input to the vampire prover.

## Ollama Install Notes

Instructions modified from: 
https://github.com/ollama/ollama/blob/main/docs/linux.md
https://ollama.com/download/linux

```
cd Programs
mkdir ollama
cd ollama
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
sudo tar -C ./ -xzf ollama-linux-amd64.tgz
cd bin
sudo chmod 777 *
```

To start a server, from the ollama/bin directory
```
./ollama serve
```


On different terminal, from the ollama/bin directory

```
./ollama -v
```

If you see a version, then it worked!

To run a specific model on the server (and download if necessary). For example:

```
./ollama run llama3.2 
```

For a list of models - https://ollama.com/library


## L2L model and vocabulary.db
The vocabulary.db file must be present, as well as the model used for language to logic conversion.

These are generated by <todo:update instructions, or link to readme>

# Running sumonlp
1. Update paths in src/config_paths.sh.
2. Run src/utils/install_requirements.sh
3. Run src/main_ui.sh


## Running on Hamming GPU Node:

From a submit node, run

srun --pty -N 1 --partition=genai --gres=gpu:1 bash
