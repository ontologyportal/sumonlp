import os
import glob
import sys
import unicodedata
import re
import spacy
from lemminflect import getInflection

import nltk
from nltk.corpus import words
from nltk.corpus import wordnet
nltk.download('words')

nltk.download('wordnet')


english_words = set(w.lower() for w in words.words())

from collections import defaultdict
from KB_reader import KB_reader


# Load spaCy English model
nlp = spacy.load("en_core_web_sm")
search_directory = os.path.expandvars("$ONTOLOGYPORTAL_GIT/sumo/WordNetMappings/")
output_file_path = os.path.expandvars("$ONTOLOGYPORTAL_GIT/sumo/supplementalTermFormats.kif")
new_termFormats = defaultdict(set)
output_file = open(output_file_path, "w", encoding="utf-8")
output_file.write(";; Access to and use of these products is governed by the GNU General Public\n")
output_file.write(";; License <http://www.gnu.org/copyleft/gpl.html>.\n")
output_file.write(";; By using these products, you agree to be bound by the terms\n")
output_file.write(";; of the GPL.\n\n")

output_file.write(";; We ask that people using or referencing this work cite our primary paper:\n")

output_file.write(";; Niles, I., and Pease, A.  2001.  Towards a Standard Upper Ontology.  In\n")
output_file.write(";; Proceedings of the 2nd International Conference on Formal Ontology in\n")
output_file.write(";; Information Systems (FOIS-2001), Chris Welty and Barry Smith, eds,\n")
output_file.write(";; Ogunquit, Maine, October 17-19, 2001.  See also www.ontologyportal.org\n\n\n")

output_file.write(";; This file was auto-generated by taking the equivalent wordnet mappings,\n")
output_file.write(";; generating a termFormat for each synset those mappings that didn't \n")
output_file.write(";; already exist.\n\n\n")
reader = KB_reader()
processes = reader.getAllSubClassesSubAttributesInstancesOf("Process")

# This only generates termFormats for equivalent terms.

def extract_synset(line):
    words = line.split()
    if len(words) < 4:
        return None

    try:
        n = 2*int(words[3], 16)
        synset = words[4:4 + n]
        synset = synset[::2]
    except (IndexError, ValueError):
        return None
    return synset

# SUMO is utf-8, but wordnet is not. We need to handle words with special characters.
# normalize tries to replace characters, like accented e, with regular e.
def clean_text(text):
    # Normalize and remove accents
    text = ''.join(
        c for c in unicodedata.normalize('NFKD', text)
        if not unicodedata.combining(c)
    )
    # Remove the Unicode replacement character
    return text.replace('ï¿½', '')

def extract_equivalent_mappings(line):
    global reader
    # Match &% followed by content, ending with =
    raw_mappings = re.findall(r"&%([^=+]+)=", line)
    if not raw_mappings:
        if line.strip().endswith('='):
            print(f"No equivalent mapping for line but ends in '=' for line: {line.strip()}")
    mappings = []
    for mapping in raw_mappings:
        if reader.existsTermInSumo(mapping):
            mappings.append(mapping)
        else:
            print("The following has a mapping to a non-existent term in SUMO and was not generated: " + line)
    return mappings if mappings else None


def print_to_file():
    global new_termFormats
    count = 0
    for term in sorted(new_termFormats):
        for termFormat in sorted(new_termFormats[term]):
            output_file.write((f"(termFormat EnglishLanguage {term} \"{termFormat}\")\n"))
            count += 1
    print(f"Created {count} new term formats, saved to: " + output_file_path)

def to_gerund(mapping, phrase):

    if mapping in ["BaseballInning", "BaseballStrike", "BaseballWalk", "Blackjack", "ChargingAFee", "ChemicalSynthesis"]:
        return phrase
    if (phrase in ["ad", "advert", "born", "ball", "recline", "recumb", "repose", "give - and - take", "extravasating"]):
        return phrase
    phrase = phrase.strip()
    # Remove 'to' if present at the start
    if phrase.lower().startswith("to "):
        phrase = phrase[3:]
    words_list = phrase.split()
    if len(words_list) == 1:
        word = words_list[0]
        if not word.lower().endswith("ing"): # getInflection will struggle with words that already end in "-ing"
            forms = getInflection(word, tag='VBG')
            if forms and forms[0] and forms[0].lower() != word.lower() and (forms[0] in english_words or wordnet.synsets(forms[0].lower())):
                return forms[0]
        # Fallback: add 'ing' and check if it's a real word
        if word.endswith('e'):
            word = word[:-1]
        elif (len(word) > 2 and
              word[-1] not in 'aeiou' and
              word[-2] in 'aeiou' and
              word[-3] not in 'aeiou'):
            word = word + word[-1]
        elif (len(word) > 4 and word.endswith("tion")):
            word = word[:-3]
        gerund = word + 'ing'
        if gerund.lower() in english_words or wordnet.synsets(gerund.lower()):
            return gerund


    # If normal way doesn't work, try using Spacy
    doc = nlp(phrase)
    first_token = doc[0]
    # Only convert if the first word is a verb and not already a gerund
    if first_token.pos_ == "VERB" and not first_token.text.lower().endswith('ing'):
        verb = first_token.text
        gerund = None
        forms = getInflection(verb, tag='VBG')
        if forms and forms[0]:
            gerund = forms[0]
        if not gerund:
            gerund = verb + 'ing'
        # Only use gerund if it's a real word
        if gerund.lower() in english_words or wordnet.synsets(gerund.lower()):
            return ' '.join([gerund] + [token.text for token in doc[1:]])
        else:
            return phrase
    else:
        return phrase

def generate_new_termFormats(mappings, synset):
    global reader, new_termFormats
    for mapping in mappings:
        mapping = mapping.strip()
        if mapping:
            for term in synset:
                term = term.replace("_", " ").strip()
                term = re.sub(r'\s*\((p|a|prenom|postnom|ip)\)', '', term) # replace syntactic markings, look it up.
                if (mapping in processes):
                    term = to_gerund(mapping, term)
                currentTermFormats = reader.term_formats[mapping]
                if not term in currentTermFormats:
                    new_termFormats[mapping].add(term)
        else:
            print("ERROR: mapping is empty.")



def process_file(file_path):
    try:
        with open(file_path, 'r', encoding='windows-1252') as in_f:
            for line_num, line in enumerate(in_f, 1):
                if line.lstrip().startswith(';'): # skip comments
                    continue

                if line.strip().endswith(('+', '=')): # sometimes there is more than one mapping. need to check.
                    line = clean_text(line)

                    synset = extract_synset(line) # If there is no term in the synset, this is an error.
                    if not synset:
                        print(f"Empty synset for line: {line.strip()}")
                        continue

                    mappings = extract_equivalent_mappings(line)
                    if not mappings: # If there is no subsuming mapping, then skip it
                        continue

                    generate_new_termFormats(mappings, synset)
    except Exception as e:
        print(f"Error processing file '{file_path}': {e}")

def find_equivalent_mappings():
    global search_directory
    if '$' in search_directory:
        search_directory = os.path.expandvars(search_directory)

    if not os.path.isdir(search_directory):
        print(f"Error: Directory '{search_directory}' not found.")
        sys.exit(1)

    file_pattern = os.path.join(search_directory, "WordNetMappings30*.txt")
    text_files = glob.glob(file_pattern)

    if not text_files:
        print(f"No text files found in '{search_directory}'.")
        sys.exit(1)

    found_count = 0
    for file_path in sorted(text_files):
        process_file(file_path)
        print("processed: " + file_path)
    print(f"Search complete. Saving to file")



find_equivalent_mappings()
print_to_file()
