2024/11/14 12:16:33 routes.go:1158: INFO server config env="map[CUDA_VISIBLE_DEVICES:0 GPU_DEVICE_ORDINAL:0 HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:55848 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/jarrad.singley/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:0 http_proxy: https_proxy: no_proxy:]"
time=2024-11-14T12:16:33.665-08:00 level=INFO source=images.go:754 msg="total blobs: 17"
time=2024-11-14T12:16:33.670-08:00 level=INFO source=images.go:761 msg="total unused blobs removed: 0"
time=2024-11-14T12:16:33.671-08:00 level=INFO source=routes.go:1205 msg="Listening on 127.0.0.1:55848 (version 0.3.13)"
time=2024-11-14T12:16:33.920-08:00 level=INFO source=common.go:135 msg="extracting embedded files" dir=/tmp/ollama2027743437/runners
time=2024-11-14T12:16:50.290-08:00 level=INFO source=common.go:49 msg="Dynamic LLM libraries" runners="[rocm_v60102 cpu cpu_avx cpu_avx2 cuda_v11 cuda_v12]"
time=2024-11-14T12:16:50.291-08:00 level=INFO source=gpu.go:199 msg="looking for compatible GPUs"
time=2024-11-14T12:16:50.677-08:00 level=INFO source=types.go:107 msg="inference compute" id=GPU-e538ce51-e6d8-ab52-f313-df7167fec7aa library=cuda variant=v12 compute=8.9 driver=12.5 name="NVIDIA L40S" total="44.4 GiB" available="44.0 GiB"
[GIN] 2024/11/14 - 12:16:50 | 200 |        60.9µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/14 - 12:16:51 | 200 |  543.559362ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/14 - 12:16:52 | 200 |       22.49µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/14 - 12:16:52 | 200 |   28.759311ms |       127.0.0.1 | POST     "/api/show"
time=2024-11-14T12:16:52.456-08:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-e538ce51-e6d8-ab52-f313-df7167fec7aa parallel=4 available=47251259392 required="3.7 GiB"
time=2024-11-14T12:16:52.456-08:00 level=INFO source=server.go:108 msg="system memory" total="2003.4 GiB" free="1984.3 GiB" free_swap="4.0 GiB"
time=2024-11-14T12:16:52.457-08:00 level=INFO source=memory.go:326 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[44.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="2.4 GiB" memory.weights.repeating="2.1 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
time=2024-11-14T12:16:52.479-08:00 level=INFO source=server.go:399 msg="starting llama server" cmd="/tmp/ollama2027743437/runners/cuda_v12/ollama_llama_server --model /home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 29 --parallel 4 --port 39649"
time=2024-11-14T12:16:52.484-08:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-11-14T12:16:52.485-08:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2024-11-14T12:16:52.485-08:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server error"
/tmp/ollama2027743437/runners/cuda_v12/ollama_llama_server: error while loading shared libraries: libcudart.so.12: cannot open shared object file: No such file or directory
time=2024-11-14T12:16:52.736-08:00 level=ERROR source=sched.go:455 msg="error loading llama server" error="llama runner process has terminated: exit status 127"
[GIN] 2024/11/14 - 12:16:52 | 500 |  621.292311ms |       127.0.0.1 | POST     "/api/generate"
time=2024-11-14T12:16:57.753-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.01700966 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2024-11-14T12:16:58.012-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.276161924 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2024-11-14T12:16:58.260-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.523772135 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
[GIN] 2024/11/14 - 12:19:09 | 200 |       23.82µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/14 - 12:19:09 | 200 |  572.345114ms |       127.0.0.1 | POST     "/api/pull"
[GIN] 2024/11/14 - 12:19:17 | 200 |       24.71µs |       127.0.0.1 | HEAD     "/"
[GIN] 2024/11/14 - 12:19:17 | 200 |   28.921719ms |       127.0.0.1 | POST     "/api/show"
time=2024-11-14T12:19:17.841-08:00 level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff gpu=GPU-e538ce51-e6d8-ab52-f313-df7167fec7aa parallel=4 available=47251259392 required="3.7 GiB"
time=2024-11-14T12:19:17.841-08:00 level=INFO source=server.go:108 msg="system memory" total="2003.4 GiB" free="1984.3 GiB" free_swap="4.0 GiB"
time=2024-11-14T12:19:17.841-08:00 level=INFO source=memory.go:326 msg="offload to cuda" layers.requested=-1 layers.model=29 layers.offload=29 layers.split="" memory.available="[44.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="3.7 GiB" memory.required.partial="3.7 GiB" memory.required.kv="896.0 MiB" memory.required.allocations="[3.7 GiB]" memory.weights.total="2.4 GiB" memory.weights.repeating="2.1 GiB" memory.weights.nonrepeating="308.2 MiB" memory.graph.full="424.0 MiB" memory.graph.partial="570.7 MiB"
time=2024-11-14T12:19:17.863-08:00 level=INFO source=server.go:399 msg="starting llama server" cmd="/tmp/ollama2027743437/runners/cuda_v12/ollama_llama_server --model /home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff --ctx-size 8192 --batch-size 512 --embedding --log-disable --n-gpu-layers 29 --parallel 4 --port 35669"
time=2024-11-14T12:19:17.863-08:00 level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2024-11-14T12:19:17.863-08:00 level=INFO source=server.go:598 msg="waiting for llama runner to start responding"
time=2024-11-14T12:19:17.863-08:00 level=INFO source=server.go:632 msg="waiting for server to become available" status="llm server error"
/tmp/ollama2027743437/runners/cuda_v12/ollama_llama_server: error while loading shared libraries: libcudart.so.12: cannot open shared object file: No such file or directory
time=2024-11-14T12:19:18.115-08:00 level=ERROR source=sched.go:455 msg="error loading llama server" error="llama runner process has terminated: exit status 127"
[GIN] 2024/11/14 - 12:19:18 | 500 |  619.245727ms |       127.0.0.1 | POST     "/api/generate"
time=2024-11-14T12:19:23.211-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.096389698 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2024-11-14T12:19:23.469-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.354046563 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
time=2024-11-14T12:19:23.729-08:00 level=WARN source=sched.go:646 msg="gpu VRAM usage didn't recover within timeout" seconds=5.614152717 model=/home/jarrad.singley/.ollama/models/blobs/sha256-dde5aa3fc5ffc17176b5e8bdc82f587b24b2678c6c66101bf7da77af9f7ccdff
